---
- name: Fail if openhpc_slurm_control_host or openhpc_cluster_name or openhpc_slurm_partitions are undefined
  fail:
    msg: "Undefined openhpc_slurm_control_host or openhpc_cluster_name or openhpc_slurm_partitions, or latter is empty."
  when:
    openhpc_slurm_control_host == none or
    openhpc_cluster_name == none or
    openhpc_slurm_partitions | length == 0

- name: Fail if configless mode selected when not on Centos 8
  fail:
    msg: "openhpc_slurm_configless = True requires Centos8 / OpenHPC v2"
  when: openhpc_slurm_configless and not ansible_distribution_major_version == "8"

- name: Ensure the Slurm spool directory exists
  file:
    path: /var/spool/slurm
    owner: slurm
    group: slurm
    mode: 0755
    state: directory

- name: Generate a Munge key
  # NB this is usually a no-op as the package install actually generates a (node-unique) one, so won't usually trigger handler
  command: "dd if=/dev/urandom of=/etc/munge/munge.key bs=1 count=1024"
  args:
    creates: "/etc/munge/munge.key"
  when: inventory_hostname == openhpc_slurm_control_host

- name: Retrieve Munge key
  slurp:
    src: "{{ '/etc/munge/munge.key' if not (openhpc_munge_key_path | default('')) else openhpc_munge_key_path }}"
  register: openhpc_munge_key
  delegate_to: "{{ openhpc_slurm_control_host if not (openhpc_munge_key_path | default('')) else 'localhost' }}"

- name: Write Munge key
  copy:
    content: "{{ openhpc_munge_key.content | b64decode }}"
    dest: "/etc/munge/munge.key"
    owner: munge
    group: munge
    mode: 0400
  notify:
    - Restart Munge service

- name: Set fact for login-only nodes
  # needed b/c openhpc_enable doesn't end up in hostvars
  set_fact:
    openhpc_login_only: "{{ openhpc_enable.login | default(false) | bool }}"

- name: Template slurmdbd.conf
  template:
    src: slurmdbd.conf.j2
    dest: /etc/slurm/slurmdbd.conf
    mode: "0600"
    owner: root
    group: root
  notify: Restart slurmdbd service
  when: openhpc_enable.database | default(false) | bool

- name: Apply customised SLURM configuration
  template:
    src: slurm.conf.j2
    dest: /etc/slurm/slurm.conf
    owner: root
    group: root
    mode: 0644
    lstrip_blocks: true
  when: openhpc_enable.control | default(false) or not openhpc_slurm_configless
  notify:
    - Reload SLURM service

- name: Set slurmctld location for configless operation
  lineinfile:
    path: /etc/sysconfig/slurmd
    line: "SLURMD_OPTIONS='--conf-server {{ openhpc_slurm_control_host }}'"
    regexp: "^SLURMD_OPTIONS="
    create: yes
    owner: root
    group: root
    mode: 0644
  when:
    - (openhpc_enable.batch | default(false) | bool) or (openhpc_enable.login | default(false) | bool)
    - openhpc_slurm_configless
  notify:
    - Reload SLURM service

# Munge state could be unchanged but the service is not running.
# Handle that here.
- name: Configure Munge service
  service:
    name: munge
    enabled: "{{ openhpc_slurm_service_enabled | bool }}"
    state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"

- name: Ensure slurmdbd is started and running
  service:
    name: slurmdbd
    enabled: "{{ openhpc_slurm_service_enabled | bool }}"
    state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"
  when: openhpc_enable.database | default(false) | bool

# In case the service isn't running and the config hasn't changed to trigger
# the handler, ensure it's running here.
- name: Configure Slurm service
  service:
    name: "{{ openhpc_slurm_service }}"
    enabled: "{{ openhpc_slurm_service_enabled | bool }}"
    state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"
  when: openhpc_slurm_service is not none
